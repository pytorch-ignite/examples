{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "convert-pytorch-to-ignite.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ---\n",
        "title: How to convert pure PyTorch code to Ignite\n",
        "weight: 2\n",
        "downloads: true\n",
        "sidebar: true\n",
        "tags:\n",
        "  - training loop\n",
        "  - validation loop\n",
        "  - metrics\n",
        "  - checkpoints\n",
        "  - progress bar\n",
        "  - logging\n",
        "--- -->\n",
        "# How to convert pure PyTorch code to Ignite "
      ],
      "metadata": {
        "id": "xo0JaCAvVI64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this guide, we will show how PyTorch code components can be converted into compact and flexible PyTorch-Ignite code. \n",
        "\n",
        "<!--more-->\n",
        "\n",
        "![Convert PyTorch to Ignite](assets/convert-pytorch2ignite.gif)"
      ],
      "metadata": {
        "id": "CXNZ4XPeV8_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since Ignite focuses on the training and validation pipeline, the code for models, datasets, optimizers, etc will remain user-defined and in pure PyTorch."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = ...\n",
        "train_loader = ...\n",
        "val_loader = ...\n",
        "optimizer = ...\n",
        "criterion = ..."
      ],
      "outputs": [],
      "metadata": {
        "id": "L6zvxAsVjP-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop to `trainer`\n",
        "\n",
        "A typical PyTorch training loop processes a single batch of data, passes it through the `model`, calculates `loss`, etc as below:\n",
        "\n",
        "```python\n",
        "for batch in train_loader:\n",
        "    model.train()\n",
        "    inputs, targets = batch\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "```"
      ],
      "metadata": {
        "id": "2EmmpiTX6huF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert the above code into Ignite we need to move the code or steps taken to process a single batch of data while training under a function (`train_step()` below). This function will take `engine` and `batch` (current batch of data) as arguments and can return any data (usually the loss) that can be accessed via `engine.state.output`. We pass this function to `Engine` which creates a `trainer` object."
      ],
      "metadata": {
        "id": "zDkeEWz58hCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from ignite.engine import Engine\n",
        "\n",
        "\n",
        "def train_step(engine, batch):\n",
        "    model.train()\n",
        "    inputs, targets = batch\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "trainer = Engine(train_step)"
      ],
      "outputs": [],
      "metadata": {
        "id": "lkWiJVuvh-LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are other [helper methods](https://pytorch.org/ignite/engine.html#helper-methods-to-define-supervised-trainer-and-evaluator) that directly create the `trainer` object without writing a custom function for some common use cases like [supervised training](https://pytorch.org/ignite/generated/ignite.engine.create_supervised_trainer.html#ignite.engine.create_supervised_trainer) and [truncated backprop through time](https://pytorch.org/ignite/contrib/engines.html#ignite.contrib.engines.tbptt.create_supervised_tbptt_trainer)."
      ],
      "metadata": {
        "id": "4MWJzKK8-AiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Loop to `evaluator`\n",
        "\n",
        "The validation loop typically makes predictions (`y_pred` below) on the `val_loader` batch by batch and uses them to calculate evaluation metrics (Accuracy, Intersection over Union, etc) as below:\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "num_correct = 0\n",
        "num_examples = 0\n",
        "\n",
        "for batch in val_loader:\n",
        "    x, y = batch\n",
        "    y_pred = model(x)\n",
        "\n",
        "    correct = torch.eq(torch.round(y_pred).type(y.type()), y).view(-1)\n",
        "    num_correct = torch.sum(correct).item()\n",
        "    num_examples = correct.shape[0]\n",
        "    print(f\"Epoch: {epoch},  Accuracy: {num_correct / num_examples}\")\n",
        "```"
      ],
      "metadata": {
        "id": "cocfuUFZ8okw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will convert this to Ignite in two steps by separating the validation and metrics logic.\n",
        "\n",
        "We will move the model evaluation logic under another function (`validation_step()` below) which receives the same parameters as `train_step()` and processes a single batch of data to return some output (usually the predicted and actual value which can be used to calculate metrics) stored in `engine.state.output`. Another instance (called `evaluator` below) of `Engine` is created by passing the `validation_step()` function."
      ],
      "metadata": {
        "id": "N0ETiWo9E0D4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def validation_step(engine, batch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x, y = batch\n",
        "        y_pred = model(x)\n",
        "\n",
        "    return y_pred, y\n",
        "    \n",
        "    \n",
        "evaluator = Engine(validation_step)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zv2kceT0CS-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the training loop, there are [helper methods](https://pytorch.org/ignite/engine.html#helper-methods-to-define-supervised-trainer-and-evaluator) to avoid writing this custom evaluation function like [`create_supervised_evaluator`](https://pytorch.org/ignite/generated/ignite.engine.create_supervised_evaluator.html#ignite.engine.create_supervised_evaluator).\n",
        "\n",
        "**Note**: You can create different evaluators for training, validation, and testing if they serve different purposes. A common practice is to have two separate evaluators for training and validation, since the results of the validation evaluator are helpful in determining the best model to save after training."
      ],
      "metadata": {
        "id": "EAIBqfFm8oqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Switch to built-in Metrics\n",
        "\n",
        "Then we can replace the code for calculating metrics like accuracy and instead use several [out-of-the-box metrics](https://pytorch.org/ignite/metrics.html#complete-list-of-metrics) that Ignite provides or write a custom one (refer [here](https://pytorch.org/ignite/metrics.html#how-to-create-a-custom-metric)). The metrics will be computed using the `evaluator`'s output. Finally, we attach these metrics to the `evaluator` by providing a key name (\"accuracy\" below) so they can be accessed via `engine.state.metrics[key_name]`."
      ],
      "metadata": {
        "id": "4t4PsYXn8ost"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from ignite.metrics import Accuracy\n",
        "\n",
        "Accuracy().attach(evaluator, \"accuracy\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "iUVAOP6kFdA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organizing code into Events and Handlers\n",
        "\n",
        "Next, we need to identify any code that is triggered when an event occurs. Examples of events can be the start of an iteration, completion of an epoch, or even the start of backprop. We already provide some predefined events (complete list [here](https://pytorch.org/ignite/generated/ignite.engine.events.Events.html#ignite.engine.events.Events)) however we can also create custom ones (refer [here](https://pytorch.org/ignite/concepts.html#custom-events)). We move the event-specific code to different handlers (named functions, lambdas, class functions) which are attached to these events and executed whenever a specific event happens. Here are some common handlers:"
      ],
      "metadata": {
        "id": "WnGK925N5AR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running `evaluator`\n",
        "\n",
        "We can convert the code that runs the `evaluator` on the training/validation/test dataset after `validate_every` epoch:\n",
        "\n",
        "```python\n",
        "if epoch % validate_every == 0:\n",
        "    # Validation logic\n",
        "```\n",
        "\n",
        "by attaching a handler to a built-in event `EPOCH_COMPLETED` like:"
      ],
      "metadata": {
        "id": "uZIdI39b-rB4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from ignite.engine import Events\n",
        "\n",
        "validate_every = 10\n",
        "\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED(every=validate_every))\n",
        "def run_validation():\n",
        "    evaluator.run(val_loader)"
      ],
      "outputs": [],
      "metadata": {
        "id": "62Z6RmfJVn7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logging metrics\n",
        "\n",
        "Similarly, we can log the validation metrics in another handler or combine it with the above handler."
      ],
      "metadata": {
        "id": "7bkte_sKb-vr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "@trainer.on(Events.EPOCH_COMPLETED(every=validate_every))\n",
        "def log_validation():\n",
        "    metrics = evaluator.state.metrics\n",
        "    print(f\"Epoch: {trainer.state.epoch},  Accuracy: {metrics['accuracy']}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZExU6_CscHyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Progress Bar\n",
        "\n",
        "We use a built-in wrapper around `tqdm` called [`ProgressBar()`](https://pytorch.org/ignite/generated/ignite.contrib.handlers.tqdm_logger.html#module-ignite.contrib.handlers.tqdm_logger)."
      ],
      "metadata": {
        "id": "sRgDrTgi5AU_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from ignite.contrib.handlers import ProgressBar\n",
        "\n",
        "ProgressBar().attach(trainer)"
      ],
      "outputs": [],
      "metadata": {
        "id": "0j79aG7ddmk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checkpointing\n",
        "\n",
        "Instead of saving all models after `checkpoint_every` epoch:\n",
        "```python\n",
        "if epoch % checkpoint_every == 0:\n",
        "    checkpoint(model, optimizer, \"checkpoint_dir\")\n",
        "```\n",
        "\n",
        "we can smartly save the best `n_saved` models (depending on `evaluator.state.metrics`), and the state of `optimizer` and `trainer` via the built-in [`Checkpoint()`](https://pytorch.org/ignite/generated/ignite.handlers.checkpoint.Checkpoint.html#checkpoint).\n"
      ],
      "metadata": {
        "id": "vkqMcVnA5AZ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from ignite.handlers import Checkpoint, DiskSaver\n",
        "\n",
        "checkpoint_every = 5\n",
        "checkpoint_dir = ...\n",
        "\n",
        "\n",
        "checkpointer = Checkpoint(\n",
        "    to_save={'model': model, 'optimizer': optimizer, 'trainer': trainer},\n",
        "    save_handler=DiskSaver(checkpoint_dir, create_dir=True), n_saved=2\n",
        ")\n",
        "trainer.add_event_handler(\n",
        "    Events.EPOCH_COMPLETED(every=checkpoint_every), checkpointer\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "VAkDj1fpoSij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run for a number of epochs\n",
        "\n",
        "Finally, instead of:\n",
        "```python\n",
        "max_epochs = ...\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "```\n",
        "we begin training on `train_loader` via:\n",
        "```python\n",
        "trainer.run(train_loader, max_epochs)\n",
        "```"
      ],
      "metadata": {
        "id": "WbByMD6xYpgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An end-to-end example implementing the above principles can be found [here](https://pytorch-ignite.ai/tutorials/getting-started/#complete-code)."
      ],
      "metadata": {}
    }
  ]
}