{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cifar10-distributed.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9NEVKMz0v5s"
      },
      "source": [
        "<!-- ---\n",
        "title: Distributed Training on CPUs, GPUs or TPUs\n",
        "date: 2021-09-18\n",
        "downloads: true\n",
        "sidebar: true\n",
        "tags:\n",
        "  - single GPU\n",
        "  - multi GPUs on a single node\n",
        "  - multi GPUs on multiple nodes\n",
        "  - TPUs on Colab\n",
        "--- -->\n",
        "# Distributed Training with Ignite on CIFAR10 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHmvDGFx10HT"
      },
      "source": [
        "This tutorial is a brief introduction on how you can do distributed training with Ignite on one or more CPUs, GPUs or TPUs. We will also introduce several helper functions and Ignite concepts (setup common training handlers, save to/ load from checkpoints, etc) which you can easily incorporate in your code.\n",
        "\n",
        "<!--more-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trJ7_a7f17pg"
      },
      "source": [
        "We will use distributed training to train a predefined redefined [ResNet18](https://pytorch.org/vision/stable/models.html#torchvision.models.resnet18) on [CIFAR10](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CIFAR10) using either of the following configurations:\n",
        "\n",
        "* Single CPU or GPU\n",
        "* Multiple CPUs or GPUs on a single node\n",
        "* Multiple GPUs on multiple nodes\n",
        "* TPUs on Google Colab\n",
        "\n",
        "The type of distributed training we will used is called data parallelism in which we:\n",
        "\n",
        ">   1. Copy the model on every GPU\n",
        ">   2. Split the dataset and fit the models on different subsets\n",
        ">   3. Communicate the gradients at each iteration to keep the models in sync\n",
        ">\n",
        "> -- <cite>[Distributed Deep Learning 101: Introduction](https://towardsdatascience.com/distributed-deep-learning-101-introduction-ebfc1bcd59d9)</cite>\n",
        "\n",
        "PyTorch provides a [torch.nn.parallel.DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) API for this task however the implementation that supports different backends + configurations is tedious. In this example, we will see how to can enable data distributed training which is adaptable to various backends in just a few lines of code alongwith:\n",
        "* Computing training and validation metrics\n",
        "* Setup logging (and connecting with ClearML)\n",
        "* Saving the best model weights\n",
        "* Setting LR Scheduler\n",
        "* Using Automatic Mixed Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWLrQ6EH4uoD"
      },
      "source": [
        "## Download Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7k6WVw5_uts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30adc41-319c-493b-f1ee-7813f846840b"
      },
      "source": [
        "!pip install pytorch-ignite\n",
        "!pip install clearml # Optional "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.6-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 232 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.6\n",
            "Collecting clearml\n",
            "  Downloading clearml-1.0.5-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting pathlib2>=2.3.0\n",
            "  Downloading pathlib2-2.3.6-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.4.7)\n",
            "Collecting pyjwt<3.0.0,>=1.6.4\n",
            "  Downloading PyJWT-2.1.0-py3-none-any.whl (16 kB)\n",
            "Collecting humanfriendly>=2.1\n",
            "  Downloading humanfriendly-9.2-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.8.2)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from clearml) (3.13)\n",
            "Requirement already satisfied: psutil>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from clearml) (5.4.8)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.23.0)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.6.0)\n",
            "Collecting furl>=2.0.0\n",
            "  Downloading furl-2.1.2-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (21.2.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from clearml) (1.19.5)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (1.24.3)\n",
            "Requirement already satisfied: Pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (7.1.2)\n",
            "Collecting orderedmultidict>=1.0.1\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (2021.5.30)\n",
            "Installing collected packages: orderedmultidict, pyjwt, pathlib2, humanfriendly, furl, clearml\n",
            "Successfully installed clearml-1.0.5 furl-2.1.2 humanfriendly-9.2 orderedmultidict-1.0.1 pathlib2-2.3.6 pyjwt-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D71VkD74he9J"
      },
      "source": [
        "## Common Configuration\n",
        "\n",
        "We maintain a `config` dictionary which can be extended or changed to store parameters required during training. We can refer back to this code when we will use these parameters later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bg7unvqhegL"
      },
      "source": [
        "config = {\n",
        "    \"seed\": 543,\n",
        "    \"data_path\": \"cifar10\",\n",
        "    \"output_path\": \"output-cifar10/\",\n",
        "    \"model\": \"resnet18\",\n",
        "    \"batch_size\": 512,\n",
        "    \"momentum\": 0.9,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"num_workers\": 2,\n",
        "    \"num_epochs\": 5,\n",
        "    \"learning_rate\": 0.4,\n",
        "    \"num_warmup_epochs\": 1,\n",
        "    \"validate_every\": 3,\n",
        "    \"checkpoint_every\": 200,\n",
        "    \"backend\": None,\n",
        "    \"resume_from\": None,\n",
        "    \"log_every_iters\": 15,\n",
        "    \"nproc_per_node\": None,\n",
        "    \"stop_iteration\": None,\n",
        "    \"with_clearml\": False,\n",
        "    \"with_amp\": False,\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzuG8QAr5Djf"
      },
      "source": [
        "## Basic Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIgzky7Q7kUk"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RVISbXd_h1F"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    Pad,\n",
        "    RandomCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    ToTensor,\n",
        ")\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "import ignite\n",
        "import ignite.distributed as idist\n",
        "from ignite.contrib.engines import common\n",
        "from ignite.contrib.handlers import PiecewiseLinear\n",
        "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.handlers import Checkpoint, DiskSaver, global_step_from_engine\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "from ignite.utils import manual_seed, setup_logger"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXpew9g3PUKI"
      },
      "source": [
        "### Logging\n",
        "\n",
        "First we pass a [`setup_logger()`](https://pytorch.org/ignite/utils.html#ignite.utils.setup_logger) object to `log_basic_info()` and log all basic information such as different versions, current configuration, `device` and `backend` used by the current process (identified by its local rank), and number of processes (world size). `idist` ([`ignite.distrubted`](https://pytorch.org/ignite/distributed.html#)) provides several utility functions like [`get_local_rank()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.get_local_rank), [`backend()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.backend), [`get_world_size()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.get_world_size), etc to make this possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g61dzVvnEVvG"
      },
      "source": [
        "def log_basic_info(logger, config):\n",
        "    logger.info(f\"Train on CIFAR10\")\n",
        "    logger.info(f\"- PyTorch version: {torch.__version__}\")\n",
        "    logger.info(f\"- Ignite version: {ignite.__version__}\")\n",
        "    if torch.cuda.is_available():\n",
        "        # explicitly import cudnn as torch.backends.cudnn can not be pickled with hvd spawning procs\n",
        "        from torch.backends import cudnn\n",
        "\n",
        "        logger.info(\n",
        "            f\"- GPU Device: {torch.cuda.get_device_name(idist.get_local_rank())}\"\n",
        "        )\n",
        "        logger.info(f\"- CUDA version: {torch.version.cuda}\")\n",
        "        logger.info(f\"- CUDNN version: {cudnn.version()}\")\n",
        "\n",
        "    logger.info(\"\\n\")\n",
        "    logger.info(\"Configuration:\")\n",
        "    for key, value in config.items():\n",
        "        logger.info(f\"\\t{key}: {value}\")\n",
        "    logger.info(\"\\n\")\n",
        "\n",
        "    if idist.get_world_size() > 1:\n",
        "        logger.info(\"\\nDistributed setting:\")\n",
        "        logger.info(f\"\\tbackend: {idist.backend()}\")\n",
        "        logger.info(f\"\\tworld size: {idist.get_world_size()}\")\n",
        "        logger.info(\"\\n\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVXiYInWikTn"
      },
      "source": [
        "Next we will take the help of `auto_` methods in `idist` to make our dataloaders, model and optimizer automatically adapt to the current configuration `backend=None` (non-distributed) or for backends like `nccl`, `gloo`, and `xla-tpu` (distributed).\n",
        "\n",
        "Note that we are free to partially use or not use `auto_` methods at all and instead can implement something custom."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r83mIS4QCdg"
      },
      "source": [
        "### Dataloaders\n",
        "\n",
        "Next we are going to download the train and test datasets in `data_path`, apply transforms to it and return them via `get_train_test_datasets()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3BKYL7XGpZL"
      },
      "source": [
        "def get_train_test_datasets(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        download = True\n",
        "    else:\n",
        "        download = True if len(os.listdir(path)) < 1 else False\n",
        "\n",
        "    train_transform = Compose(\n",
        "        [\n",
        "            Pad(4),\n",
        "            RandomCrop(32, fill=128),\n",
        "            RandomHorizontalFlip(),\n",
        "            ToTensor(),\n",
        "            Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ]\n",
        "    )\n",
        "    test_transform = Compose(\n",
        "        [\n",
        "            ToTensor(),\n",
        "            Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    train_ds = datasets.CIFAR10(\n",
        "        root=path, train=True, download=download, transform=train_transform\n",
        "    )\n",
        "    test_ds = datasets.CIFAR10(\n",
        "        root=path, train=False, download=False, transform=test_transform\n",
        "    )\n",
        "\n",
        "    return train_ds, test_ds"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GkEcAYiRbgO"
      },
      "source": [
        "Now we have to make sure only the main process (`rank = 0`) downloads the datasets to prevent the sub processes (`rank > 0`) from downloading the same file to the same path at the same time. This way all sub processes get a copy of this already downloaded dataset. For this we use [`idist.barrier()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.barrier) to make the sub processes wait until the main process downloads the datasets. Once that is done, all the subprocesses instantiate `train_dataset` and `test_dataset`, while the main process waits. Finally, all the processes are synced up.\n",
        "\n",
        "We finally pass our dataset to [`auto_dataloader()`](https://pytorch.org/ignite/generated/ignite.distributed.auto.auto_dataloader.html#ignite.distributed.auto.auto_dataloader)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rNV-UDwRPtO"
      },
      "source": [
        "def get_dataflow(config):\n",
        "    if idist.get_local_rank() > 0:\n",
        "        idist.barrier()\n",
        "\n",
        "    train_dataset, test_dataset = get_train_test_datasets(config[\"data_path\"])\n",
        "\n",
        "    if idist.get_local_rank() == 0:\n",
        "        idist.barrier()\n",
        "\n",
        "    train_loader = idist.auto_dataloader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        num_workers=config[\"num_workers\"],\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_loader = idist.auto_dataloader(\n",
        "        test_dataset,\n",
        "        batch_size=2 * config[\"batch_size\"],\n",
        "        num_workers=config[\"num_workers\"],\n",
        "        shuffle=False,\n",
        "    )\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNLvDK-cS2sH"
      },
      "source": [
        "### Model\n",
        "\n",
        "We check if the model given in `config` is present in [torchvision.models](https://pytorch.org/vision/stable/models.html), change the last layer to output 10 classes (as present in CIFAR10) and pass it to [`auto_model()`](https://pytorch.org/ignite/generated/ignite.distributed.auto.auto_model.html#auto-model) which makes it automatically adaptable for non-distributed and distributed configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toShlIcW5oFd"
      },
      "source": [
        "def get_model(config):\n",
        "    model_name = config[\"model\"]\n",
        "    if model_name in models.__dict__:\n",
        "        fn = models.__dict__[model_name]\n",
        "    else:\n",
        "        raise RuntimeError(f\"Unknown model name {model_name}\")\n",
        "\n",
        "    model = idist.auto_model(fn(num_classes=10))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V14TfyCT8jQW"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "Then we can setup the optimizer using hyperameters from `config` and pass it through [`auto_optim()`](https://pytorch.org/ignite/generated/ignite.distributed.auto.auto_optim.html#ignite.distributed.auto.auto_optim)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iddv29eh5qU9"
      },
      "source": [
        "def get_optimizer(config, model):\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=config[\"learning_rate\"],\n",
        "        momentum=config[\"momentum\"],\n",
        "        weight_decay=config[\"weight_decay\"],\n",
        "        nesterov=True,\n",
        "    )\n",
        "    optimizer = idist.auto_optim(optimizer)\n",
        "\n",
        "    return optimizer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI0o7hgr8l9q"
      },
      "source": [
        "### Criterion\n",
        "\n",
        "We put the loss function on `device`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVDKkYqS5siE"
      },
      "source": [
        "def get_criterion():\n",
        "    return nn.CrossEntropyLoss().to(idist.device())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9No0Ockx8oRC"
      },
      "source": [
        "### LR Scheduler\n",
        "\n",
        "We will use [PiecewiseLinear](https://pytorch.org/ignite/generated/ignite.handlers.param_scheduler.PiecewiseLinear.html#ignite.handlers.param_scheduler.PiecewiseLinear) which is one of the [various LR Schedulers](https://pytorch.org/ignite/handlers.html#parameter-scheduler) Ignite provides.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcYuVeYic_e7"
      },
      "source": [
        "def get_lr_scheduler(config, optimizer):\n",
        "    milestones_values = [\n",
        "        (0, 0.0),\n",
        "        (config[\"num_iters_per_epoch\"] * config[\"num_warmup_epochs\"], config[\"learning_rate\"]),\n",
        "        (config[\"num_iters_per_epoch\"] * config[\"num_epochs\"], 0.0),\n",
        "    ]\n",
        "    lr_scheduler = PiecewiseLinear(\n",
        "        optimizer, param_name=\"lr\", milestones_values=milestones_values\n",
        "    )\n",
        "    return lr_scheduler"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjVYZdn49PKD"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp6sWINP9CAq"
      },
      "source": [
        "### Save Models\n",
        "\n",
        "We can create checkpoints using either of the two handlers:\n",
        "\n",
        "1. If `with-clearml=True`, we will save the models in ClearML's File Server using [`ClearMLSaver()`](https://pytorch.org/ignite/generated/ignite.contrib.handlers.clearml_logger.html#ignite.contrib.handlers.clearml_logger.ClearMLSaver).\n",
        "2. Else save the models to disk using [`DiskSaver()`](https://pytorch.org/ignite/generated/ignite.handlers.DiskSaver.html#ignite.handlers.DiskSaver)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DG0Pj4pJJFw"
      },
      "source": [
        "def get_save_handler(config):\n",
        "    if config[\"with_clearml\"]:\n",
        "        from ignite.contrib.handlers.clearml_logger import ClearMLSaver\n",
        "\n",
        "        return ClearMLSaver(dirname=config[\"output_path\"])\n",
        "\n",
        "    return DiskSaver(config[\"output_path\"], require_empty=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1N1iR2f9R0n"
      },
      "source": [
        "### Resume from Checkpoint\n",
        "\n",
        "If a checkpoint file path is provided, we can resume training from there by loading the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9La55z97PVa"
      },
      "source": [
        "def load_checkpoint(resume_from):\n",
        "    checkpoint_fp = Path(resume_from)\n",
        "    assert (\n",
        "        checkpoint_fp.exists()\n",
        "    ), f\"Checkpoint '{checkpoint_fp.as_posix()}' is not found\"\n",
        "    logger.info(f\"Resume from a checkpoint: {checkpoint_fp.as_posix()}\")\n",
        "    checkpoint = torch.load(checkpoint_fp.as_posix(), map_location=\"cpu\")\n",
        "    return checkpoint"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBd6WDVE_KmO"
      },
      "source": [
        "### Create Trainer\n",
        "\n",
        "Finally, we can create our `trainer` in four steps:\n",
        "1. Choose whether to enable [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html) (AMP) or not. Enabling AMP will speed up computations on large neural networks and reduce memory usage while retaining performance. A [`GradScaler()`](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler) object also has be created to scale the `loss` so that gradients don't round-up to zero while training (`scaler=True`). \n",
        "2. Create a `trainer` object using [`create_supervised_trainer()`](https://pytorch.org/ignite/generated/ignite.engine.create_supervised_trainer.html#ignite.engine.create_supervised_trainer) which internally defines the steps taken to process a single batch:\n",
        "  1. Move the batch to `device` used in current distributed configuration.\n",
        "  2. Put `model` in `train()` mode.\n",
        "  3. Perform forward pass by passing the inputs through the `model` and calculating `loss`. If AMP is enabled then this step happens with [`autocast`](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast) on which allows this step to run in mixed precision.\n",
        "  4. Perform backward pass. If AMP is enabled, then the losses will be [scaled](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.scale) before calling `backward()`, `step()` the optimizer while discarding batches that contain NaNs and [update()](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.update) the scale for the next iteration.\n",
        "  5. Store the loss as `batch loss` in `state.output`.\n",
        "3. Setup some common Ignite training handlers. You can do this individually or use [setup_common_training_handlers()](https://pytorch.org/ignite/contrib/engines.html#ignite.contrib.engines.common.setup_common_training_handlers) that takes the `trainer` and a subset of the dataset (`train_sampler`) alongwith:\n",
        "  * A dictionary mapping on what to save in the checkpoint (`to_save`) and how often (`save_every_iters`).\n",
        "  * The LR Scheduler\n",
        "  * The output of `train_step()`\n",
        "  * Other handlers\n",
        "4. If `resume_from` file path is provided, load the states of objects `to_save` from the checkpoint file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptmfSvESbEPE"
      },
      "source": [
        "def create_trainer(\n",
        "    model, optimizer, criterion, lr_scheduler, train_sampler, config, logger\n",
        "):\n",
        "\n",
        "    device = idist.device()\n",
        "    amp_mode = None\n",
        "    scaler = False\n",
        "\n",
        "    if config[\"with_amp\"]:\n",
        "        amp_mode = \"amp\" \n",
        "        scaler = True\n",
        "        \n",
        "    trainer = create_supervised_trainer(\n",
        "        model,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        device=device,\n",
        "        non_blocking=True,\n",
        "        output_transform=lambda x, y, y_pred, loss: {\"batch loss\": loss.item()},\n",
        "        amp_mode=amp_mode,\n",
        "        scaler=False,\n",
        "    )\n",
        "    trainer.logger = logger\n",
        "\n",
        "    to_save = {\n",
        "        \"trainer\": trainer,\n",
        "        \"model\": model,\n",
        "        \"optimizer\": optimizer,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "    }\n",
        "    metric_names = [\n",
        "        \"batch loss\",\n",
        "    ]\n",
        "\n",
        "    common.setup_common_training_handlers(\n",
        "        trainer=trainer,\n",
        "        train_sampler=train_sampler,\n",
        "        to_save=to_save,\n",
        "        save_every_iters=config[\"checkpoint_every\"],\n",
        "        save_handler=get_save_handler(config),\n",
        "        lr_scheduler=lr_scheduler,\n",
        "        output_names=metric_names if config[\"log_every_iters\"] > 0 else None,\n",
        "        with_pbars=False,\n",
        "        clear_cuda_cache=False,\n",
        "    )\n",
        "\n",
        "    if config[\"resume_from\"] is not None:\n",
        "        checkpoint = load_checkpoint(config[\"resume_from\"])\n",
        "        Checkpoint.load_objects(to_load=to_save, checkpoint=checkpoint)\n",
        "\n",
        "    return trainer"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ius6EM9aiG"
      },
      "source": [
        "## Evaluator\n",
        "\n",
        "The evaluator will be created via [`create_supervised_evaluator()`](https://pytorch.org/ignite/generated/ignite.engine.create_supervised_evaluator.html#ignite.engine.create_supervised_evaluator) which internally will:\n",
        "1. Set the `model` to `eval()` mode.\n",
        "2. Move the batch to `device` used in current distributed configuration.\n",
        "3. Perform forward pass. If AMP is enabled, `autocast` will be on.\n",
        "4. Store the predictions and labels in `state.output` to compute metrics.\n",
        "\n",
        "Finally, we will attach relevant Ignite metrics to the `evaluator`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPVzU6CqNlE4"
      },
      "source": [
        "def create_evaluator(model, metrics, config):\n",
        "    device = idist.device()\n",
        "\n",
        "    amp_mode = \"amp\" if config[\"with_amp\"] else None\n",
        "    evaluator = create_supervised_evaluator(\n",
        "        model, metrics=metrics, device=device, non_blocking=True, amp_mode=amp_mode\n",
        "    )\n",
        "\n",
        "    for name, metric in metrics.items():\n",
        "        metric.attach(evaluator, name)\n",
        "\n",
        "    return evaluator"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opzRc2DR9dz-"
      },
      "source": [
        "## Training\n",
        "\n",
        "Before we begin training, we must setup a few things on the master process (`rank` = 0):\n",
        "* Create folder to store checkpoints, best models and output of tensorboard logging in the format - model_backend_rank_time.\n",
        "* Log `device` name.\n",
        "* If ClearML FileServer is used to save models, then a `Task` has to be created, and we pass our `config` dictionary and the specific hyper parameters that are part of the experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_ahtRZ_i-k8"
      },
      "source": [
        "def setup_rank_zero(logger, local_rank, config):\n",
        "    device = idist.device()\n",
        "\n",
        "    if config[\"stop_iteration\"] is None:\n",
        "        now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    else:\n",
        "        now = f\"stop-on-{config['stop_iteration']}\"\n",
        "\n",
        "    output_path = config[\"output_path\"]\n",
        "    folder_name = (\n",
        "        f\"{config['model']}_backend-{idist.backend()}-{idist.get_world_size()}_{now}\"\n",
        "    )\n",
        "    output_path = Path(output_path) / folder_name\n",
        "    if not output_path.exists():\n",
        "        output_path.mkdir(parents=True)\n",
        "    config[\"output_path\"] = output_path.as_posix()\n",
        "    logger.info(f\"Output path: {config['output_path']}\")\n",
        "\n",
        "    if \"cuda\" in device.type:\n",
        "        config[\"cuda device name\"] = torch.cuda.get_device_name(local_rank)\n",
        "\n",
        "    if config[\"with_clearml\"]:\n",
        "        try:\n",
        "            from clearml import Task\n",
        "        except ImportError:\n",
        "            # Backwards-compatibility for legacy Trains SDK\n",
        "            from trains import Task\n",
        "\n",
        "        task = Task.init(\"CIFAR10-Training\", task_name=output_path.stem)\n",
        "        task.connect_configuration(config)\n",
        "        # Log hyper parameters\n",
        "        hyper_params = [\n",
        "            \"model\",\n",
        "            \"batch_size\",\n",
        "            \"momentum\",\n",
        "            \"weight_decay\",\n",
        "            \"num_epochs\",\n",
        "            \"learning_rate\",\n",
        "            \"num_warmup_epochs\",\n",
        "        ]\n",
        "        task.connect({k: v for k, v in config.items()})"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QT7tiAcJEk0"
      },
      "source": [
        "This is a standard utility function to log `train` and `val` metrics after `validate_every` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hqmFjOJN8kK"
      },
      "source": [
        "def log_metrics(logger, epoch, elapsed, tag, metrics):\n",
        "    metrics_output = \"\\n\".join([f\"\\t{k}: {v}\" for k, v in metrics.items()])\n",
        "    logger.info(\n",
        "        f\"\\nEpoch {epoch} - Evaluation time (seconds): {elapsed:.2f} - {tag} metrics:\\n {metrics_output}\"\n",
        "    )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fKzNAa1JVRR"
      },
      "source": [
        "This is where the main logic resides, i.e., we will call all the above functions from within here:\n",
        "1. Basic Setup\n",
        "  1. We set a [`manual_seed()`]() and [`setup_logger`()](), then log all basic information.\n",
        "  2. Initialise `dataloaders`, `model`, `optimizer`, `criterion` and `lr_scheduler`.\n",
        "2. We use the above objects to create a `trainer`.\n",
        "3. Evaluator\n",
        "  1. Define some relevant Ignite metrics like [`Accuracy()`](https://pytorch.org/ignite/generated/ignite.metrics.Accuracy.html#accuracy) and [`Loss()`](https://pytorch.org/ignite/generated/ignite.metrics.Loss.html#loss).\n",
        "  2. Create two evaluators: `train_evaluator` and `val_evaluator` to compute metrics on the `train_dataloader` and `val_dataloader` respectively, however `val_evaluator` will store the best models based on validation metrics.\n",
        "  3. Define `run_validation()` to compute metrics on both dataloaders and log them. Then we attach this function to `trainer` to run after `validate_every` epochs and after training is complete.\n",
        "4. Setup TensorBoard logging using [`setup_tb_logging()`](https://pytorch.org/ignite/contrib/engines.html#ignite.contrib.engines.common.setup_tb_logging) on the master process for the trainer and evaluators so that training and validation metrics along with the learning rate can be logged.\n",
        "5. Define a [`Checkpoint()`](https://pytorch.org/ignite/generated/ignite.handlers.checkpoint.Checkpoint.html#ignite.handlers.checkpoint.Checkpoint) object to store the two best models (`n_saved`) by validation accuracy (defined in `metrics` as `Accuracy()`) and attach it to `val_evaluator` so that it can be executed everytime `val_evaluator` runs.\n",
        "6. Stop training, if given, once it reaches `stop_iteration` using [`terminate()`](https://pytorch.org/ignite/generated/ignite.engine.engine.Engine.html#ignite.engine.engine.Engine.terminate).\n",
        "7. Try training on `train_loader` for `num_epochs`\n",
        "8. Close Tensorboard logger once training is completed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3l1U7_vL8pg"
      },
      "source": [
        "def training(local_rank, config):\n",
        "\n",
        "    rank = idist.get_rank()\n",
        "    manual_seed(config[\"seed\"] + rank)\n",
        "\n",
        "    logger = setup_logger(name=\"CIFAR10-Training\")\n",
        "    log_basic_info(logger, config)\n",
        "\n",
        "    if rank == 0:\n",
        "        setup_rank_zero(logger, local_rank, config)\n",
        "\n",
        "    train_loader, val_loader = get_dataflow(config)\n",
        "    model = get_model(config)\n",
        "    optimizer = get_optimizer(config, model)\n",
        "    criterion = get_criterion()\n",
        "    config[\"num_iters_per_epoch\"] = len(train_loader)\n",
        "    lr_scheduler = get_lr_scheduler(config, optimizer)\n",
        "\n",
        "    trainer = create_trainer(\n",
        "        model, optimizer, criterion, lr_scheduler, train_loader.sampler, config, logger\n",
        "    )\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": Accuracy(),\n",
        "        \"Loss\": Loss(criterion),\n",
        "    }\n",
        "\n",
        "    train_evaluator = create_evaluator(model, metrics, config)\n",
        "    val_evaluator = create_evaluator(model, metrics, config)\n",
        "\n",
        "    def run_validation(engine):\n",
        "        epoch = trainer.state.epoch\n",
        "        state = train_evaluator.run(train_loader)\n",
        "        log_metrics(logger, epoch, state.times[\"COMPLETED\"], \"train\", state.metrics)\n",
        "        state = val_evaluator.run(val_loader)\n",
        "        log_metrics(logger, epoch, state.times[\"COMPLETED\"], \"val\", state.metrics)\n",
        "\n",
        "    trainer.add_event_handler(\n",
        "        Events.EPOCH_COMPLETED(every=config[\"validate_every\"]) | Events.COMPLETED,\n",
        "        run_validation,\n",
        "    )\n",
        "\n",
        "    if rank == 0:\n",
        "        evaluators = {\"train\": train_evaluator, \"val\": val_evaluator}\n",
        "        tb_logger = common.setup_tb_logging(\n",
        "            config[\"output_path\"], trainer, optimizer, evaluators=evaluators\n",
        "        )\n",
        "\n",
        "    best_model_handler = Checkpoint(\n",
        "        {\"model\": model},\n",
        "        get_save_handler(config),\n",
        "        filename_prefix=\"best\",\n",
        "        n_saved=2,\n",
        "        global_step_transform=global_step_from_engine(trainer),\n",
        "        score_name=\"val_accuracy\",\n",
        "        score_function=Checkpoint.get_default_score_fn(\"Accuracy\"),\n",
        "    )\n",
        "    val_evaluator.add_event_handler(\n",
        "        Events.COMPLETED,\n",
        "        best_model_handler,\n",
        "    )\n",
        "\n",
        "    if config[\"stop_iteration\"] is not None:\n",
        "\n",
        "        @trainer.on(Events.ITERATION_STARTED(once=config[\"stop_iteration\"]))\n",
        "        def _():\n",
        "            logger.info(f\"Stop training on {trainer.state.iteration} iteration\")\n",
        "            trainer.terminate()\n",
        "\n",
        "    try:\n",
        "        trainer.run(train_loader, max_epochs=config[\"num_epochs\"])\n",
        "    except Exception as e:\n",
        "        logger.exception(\"\")\n",
        "        raise e\n",
        "\n",
        "    if rank == 0:\n",
        "        tb_logger.close()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WioiRM5U9ipQ"
      },
      "source": [
        "## Run with Different Configurations\n",
        "\n",
        "We can easily run the above code with the context manager [Parallel](https://pytorch.org/ignite/generated/ignite.distributed.launcher.Parallel.html#ignite.distributed.launcher.Parallel) which will spawn `nproc_per_node` processes according to the specfied `backend`.\n",
        "\n",
        "You can select either of the following configurations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhmj6meIMk2w"
      },
      "source": [
        "spawn_kwargs = {}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99enig2IdNiF"
      },
      "source": [
        "### Single CPU or GPU\n",
        "\n",
        "If on Colab, go to Runtime > Change runtime type and select Hardware accelerator = None for CPU or GPU for cuda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trrSIC-Qot4Z",
        "outputId": "4756994a-d19d-41e5-8d2f-46f0dbc5b8ba"
      },
      "source": [
        "spawn_kwargs[\"nproc_per_node\"] = None\n",
        "\n",
        "with idist.Parallel(backend=config[\"backend\"], **spawn_kwargs) as parallel:\n",
        "    parallel.run(training, config)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-03 07:27:38,108 ignite.distributed.launcher.Parallel INFO: - Run '<function training at 0x7fd9a71f2050>' in 1 processes\n",
            "2021-09-03 07:27:38,114 CIFAR10-Training INFO: Train on CIFAR10\n",
            "2021-09-03 07:27:38,116 CIFAR10-Training INFO: - PyTorch version: 1.9.0+cu102\n",
            "2021-09-03 07:27:38,118 CIFAR10-Training INFO: - Ignite version: 0.4.6\n",
            "2021-09-03 07:27:38,120 CIFAR10-Training INFO: - GPU Device: Tesla K80\n",
            "2021-09-03 07:27:38,123 CIFAR10-Training INFO: - CUDA version: 10.2\n",
            "2021-09-03 07:27:38,125 CIFAR10-Training INFO: - CUDNN version: 7605\n",
            "2021-09-03 07:27:38,127 CIFAR10-Training INFO: \n",
            "\n",
            "2021-09-03 07:27:38,129 CIFAR10-Training INFO: Configuration:\n",
            "2021-09-03 07:27:38,131 CIFAR10-Training INFO: \tseed: 543\n",
            "2021-09-03 07:27:38,134 CIFAR10-Training INFO: \tdata_path: cifar10\n",
            "2021-09-03 07:27:38,137 CIFAR10-Training INFO: \toutput_path: output-cifar10/resnet18_backend-None-1_20210903-072555\n",
            "2021-09-03 07:27:38,139 CIFAR10-Training INFO: \tmodel: resnet18\n",
            "2021-09-03 07:27:38,141 CIFAR10-Training INFO: \tbatch_size: 512\n",
            "2021-09-03 07:27:38,144 CIFAR10-Training INFO: \tmomentum: 0.9\n",
            "2021-09-03 07:27:38,146 CIFAR10-Training INFO: \tweight_decay: 0.0001\n",
            "2021-09-03 07:27:38,149 CIFAR10-Training INFO: \tnum_workers: 2\n",
            "2021-09-03 07:27:38,150 CIFAR10-Training INFO: \tnum_epochs: 5\n",
            "2021-09-03 07:27:38,153 CIFAR10-Training INFO: \tlearning_rate: 0.4\n",
            "2021-09-03 07:27:38,155 CIFAR10-Training INFO: \tnum_warmup_epochs: 1\n",
            "2021-09-03 07:27:38,157 CIFAR10-Training INFO: \tvalidate_every: 3\n",
            "2021-09-03 07:27:38,159 CIFAR10-Training INFO: \tcheckpoint_every: 200\n",
            "2021-09-03 07:27:38,160 CIFAR10-Training INFO: \tbackend: None\n",
            "2021-09-03 07:27:38,162 CIFAR10-Training INFO: \tresume_from: None\n",
            "2021-09-03 07:27:38,164 CIFAR10-Training INFO: \tlog_every_iters: 15\n",
            "2021-09-03 07:27:38,165 CIFAR10-Training INFO: \tnproc_per_node: None\n",
            "2021-09-03 07:27:38,167 CIFAR10-Training INFO: \tstop_iteration: None\n",
            "2021-09-03 07:27:38,169 CIFAR10-Training INFO: \twith_clearml: False\n",
            "2021-09-03 07:27:38,171 CIFAR10-Training INFO: \twith_amp: False\n",
            "2021-09-03 07:27:38,172 CIFAR10-Training INFO: \tcuda device name: Tesla K80\n",
            "2021-09-03 07:27:38,174 CIFAR10-Training INFO: \tnum_iters_per_epoch: 97\n",
            "2021-09-03 07:27:38,176 CIFAR10-Training INFO: \n",
            "\n",
            "2021-09-03 07:27:38,178 CIFAR10-Training INFO: Output path: output-cifar10/resnet18_backend-None-1_20210903-072555/resnet18_backend-None-1_20210903-072738\n",
            "2021-09-03 07:27:39,357 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': \n",
            "\t{'batch_size': 512, 'num_workers': 2, 'shuffle': True, 'drop_last': True, 'pin_memory': True}\n",
            "2021-09-03 07:27:39,359 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': \n",
            "\t{'batch_size': 1024, 'num_workers': 2, 'shuffle': False, 'pin_memory': True}\n",
            "2021-09-03 07:27:41,494 CIFAR10-Training INFO: Engine run starting with max_epochs=5.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "2021-09-03 07:29:42,929 CIFAR10-Training INFO: Epoch[1] Complete. Time taken: 00:02:01\n",
            "2021-09-03 07:31:44,130 CIFAR10-Training INFO: Epoch[2] Complete. Time taken: 00:02:01\n",
            "2021-09-03 07:34:05,515 CIFAR10-Training INFO: \n",
            "Epoch 3 - Evaluation time (seconds): 19.90 - train metrics:\n",
            " \tAccuracy: 0.5890786082474226\n",
            "\tLoss: 1.1348281545737355\n",
            "2021-09-03 07:34:08,479 CIFAR10-Training INFO: \n",
            "Epoch 3 - Evaluation time (seconds): 2.92 - val metrics:\n",
            " \tAccuracy: 0.5996\n",
            "\tLoss: 1.1120291015625\n",
            "2021-09-03 07:34:08,481 CIFAR10-Training INFO: Epoch[3] Complete. Time taken: 00:02:24\n",
            "2021-09-03 07:36:09,371 CIFAR10-Training INFO: Epoch[4] Complete. Time taken: 00:02:01\n",
            "2021-09-03 07:38:10,543 CIFAR10-Training INFO: Epoch[5] Complete. Time taken: 00:02:01\n",
            "2021-09-03 07:38:30,605 CIFAR10-Training INFO: \n",
            "Epoch 5 - Evaluation time (seconds): 20.02 - train metrics:\n",
            " \tAccuracy: 0.6951312822164949\n",
            "\tLoss: 0.8538916676314836\n",
            "2021-09-03 07:38:33,587 CIFAR10-Training INFO: \n",
            "Epoch 5 - Evaluation time (seconds): 2.94 - val metrics:\n",
            " \tAccuracy: 0.703\n",
            "\tLoss: 0.8472966796875\n",
            "2021-09-03 07:38:33,590 CIFAR10-Training INFO: Engine run complete. Time taken: 00:10:52\n",
            "2021-09-03 07:38:33,643 ignite.distributed.launcher.Parallel INFO: End of run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6s0Rn_mdewW"
      },
      "source": [
        "### TPUs on Colab\n",
        "\n",
        "Go to Runtime > Change runtime type and select Hardware accelerator = TPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6e2NW3kofu8"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "spawn_kwargs[\"nproc_per_node\"] = 4\n",
        "spawn_kwargs[\"start_method\"] = \"fork\"\n",
        "config[\"backend\"] = \"gloo\"\n",
        "\n",
        "with idist.Parallel(backend=config[\"backend\"], **spawn_kwargs) as parallel:\n",
        "    parallel.run(training, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nxfg7j7deRD"
      },
      "source": [
        "### Single Node, Multiple GPUs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlVEECk_kNs7"
      },
      "source": [
        "spawn_kwargs[\"nproc_per_node\"] = 2 # For 2 GPUs\n",
        "config[\"backend\"] = \"nccl\" # Find out all supported backends via ignite.distributed.utils.available_backends()\n",
        "\n",
        "with idist.Parallel(backend=config[\"backend\"], **spawn_kwargs) as parallel:\n",
        "    parallel.run(training, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoLHt5LxlT-C"
      },
      "source": [
        "If you want to run the above code as a script (`main.py`):\n",
        "\n",
        "* Using [torch.distributed.launch]()\n",
        "```bash\n",
        "python -m torch.distributed.launch --use_env main.py\n",
        "```\n",
        "\n",
        "* Using [horovod]()\n",
        "```bash\n",
        "horovodrun -np=2 python main.py\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-jBxovGdee1"
      },
      "source": [
        "### Multiple Nodes, Multiple GPUs\n",
        "\n",
        "For running on 2 nodes, with 8 GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4F3iemXpd28"
      },
      "source": [
        "spawn_kwargs = {\n",
        "    \"nproc_per_node\": 8,\n",
        "    \"nnodes\": 2,\n",
        "    \"node_rank\": 0,\n",
        "    \"master_addr\": \"master\",\n",
        "    \"master_port\": 15000\n",
        "}\n",
        "config[\"backend\"] = \"nccl\"\n",
        "\n",
        "with idist.Parallel(backend=config[\"backend\"], **spawn_kwargs) as parallel:\n",
        "    parallel.run(training, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f9F4JTAnMJh"
      },
      "source": [
        "This needs to be run on all nodes, hence the `node_rank` needs to change. It's better to run the above code as a script by changing `spawn_kwargs` to:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAowcpEWqx7L"
      },
      "source": [
        "spawn_kwargs = {\n",
        "    \"nproc_per_node\": 8,\n",
        "    \"nnodes\": 2,\n",
        "    \"node_rank\": args.node_rank,\n",
        "    \"master_addr\": \"master\",\n",
        "    \"master_port\": 15000\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYvU2gaqqzTp"
      },
      "source": [
        "And then run as follows on all nodes via changing `node_rank`:\n",
        "\n",
        "```bash\n",
        "python main.py --node_rank=0\n",
        "```\n",
        "\n",
        "You can also use `torch.distributed.launch` and `horovod` as provided in Single Node, Multiple GPUs to run your script:\n",
        "\n",
        "```bash\n",
        "python -m torch.distributed.launch --node_rank=0 --use_env main.py\n",
        "```"
      ]
    }
  ]
}