{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4G9j3D6Qk4jV"
   },
   "source": [
    "# Machine Translation using PyTorch Ignite\n",
    "This tutorial is a brief introduction on how you can train a machine translation model (or any other seq2seq model) using PyTorch Ignite. \n",
    "\n",
    "This notebook uses Models, Dataset and Tokenizers from Huggingface, hence they can be easily replaced by other models from the ðŸ¤— Hub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWLrQ6EH4uoD"
   },
   "source": [
    "## Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OUsi2Pv957NV"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/pytorch/ignite.git@master;\n",
    "!pip install git+https://github.com/huggingface/transformers.git@master;\n",
    "!pip install git+https://github.com/huggingface/datasets.git@master;\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0c2e4I4FWoT"
   },
   "source": [
    "### For TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Gg74Dc3UFUcG"
   },
   "outputs": [],
   "source": [
    "# VERSION = !curl -s https://api.github.com/repos/pytorch/xla/releases/latest | grep -Po '\"tag_name\": \"v\\K.*?(?=\")'\n",
    "# VERSION = VERSION[0].rstrip('.0') # remove trailing zero\n",
    "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-{VERSION}-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RT4inGswl_oz"
   },
   "source": [
    "## Common Configuration\n",
    "We maintain a config dictionary which can be extended or changed to store parameters required during training. We can refer back to this code when we will use these parameters later.\n",
    "\n",
    "In this example we are using mbart-large-cc25, which has 610M parameters. Hence we train it by keeping the weights of the encoder frozen due to limited GPU capacity. If you have more GPU capacity, the encoder weights can be unfrozen using the ``freeze_encoder`` and ``batch_size`` can be kept higher.\n",
    "\n",
    "Also here we train on less number of iterations per step, this can be modified using the ``epoch_length`` config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5SInIPWqQmuq"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\": 216,\n",
    "    \"with_amp\": False,\n",
    "    \"num_epochs\": 2,\n",
    "    \"batch_size\": 6,\n",
    "    \"output_path_\": \"/content\",\n",
    "    \"model_name\": \"t5-base\",\n",
    "    \"tokenizer_name\": \"t5-base\",\n",
    "    \"freeze_encoder\": False,\n",
    "    \"num_workers\": 4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"accumulation_steps\": 1,\n",
    "    \"epoch_length\": 50000,\n",
    "    \"print_output_every\": 50,\n",
    "}\n",
    "\n",
    "dataset_configs = {\n",
    "    \"source_language\":\"English\",\n",
    "    \"source_text_id\":\"en\",\n",
    "    \"target_language\":\"German\",\n",
    "    \"target_text_id\":\"de\",\n",
    "    \"max_length\": 80,\n",
    "    \"train_dataset_length\": -1,\n",
    "    \"validation_dataset_length\": 2000,\n",
    "    \"train_test_split\": 0.3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzuG8QAr5Djf"
   },
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIgzky7Q7kUk"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MNj9cx216vqT"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import ignite\n",
    "import ignite.distributed as idist\n",
    "from ignite.contrib.engines import common\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import Checkpoint, DiskSaver, global_step_from_engine\n",
    "from ignite.metrics import Bleu\n",
    "from ignite.utils import manual_seed, setup_logger\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwpsSs-ef3Cc"
   },
   "source": [
    "### Preparing data\n",
    "\n",
    "We will be using the [new_commentary](https://github.com/huggingface/datasets/blob/master/datasets/news_commentary/news_commentary.py) data (English - German) from the ðŸ¤— Hub for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "8f865438773d4c9ca8c12cae83df79b4",
      "450f239f4b83439ebef94e44e655b41c",
      "8ab0ef2ca6a7481aa3bb1561ac179674",
      "c0157213cc6e44f6aab3f8087157d6b5",
      "cf1aff588ac34b97a0ecd46a11ca961c",
      "2bc960960a7b4a71bf723767558b06fe",
      "3e27f0b174a44917b2081d88c31177c6",
      "2de127df69f546f2b8ffe1b729b747ac",
      "09299b671401414897a169c8ca6fab5c",
      "440e4631c3ec4673b8378513cdb5ef88",
      "8e63d03c0e014272b66cfd25d1cba5ea"
     ]
    },
    "id": "GjLlDIzR6QjG",
    "outputId": "c8c0b345-f7e4-4d49-e900-073ba5307535"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset news_commentary (/root/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f865438773d4c9ca8c12cae83df79b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-199f0b60779b6122.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"news_commentary\", \"de-en\")\n",
    "dataset = dataset.shuffle(seed=config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIVWhVBzJrXT",
    "outputId": "0b60a5a2-82ed-409a-a9a7-fd6749b60b3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-23d286abe396b3d4.arrow and /root/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-387687cf22f2e607.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths\n",
      "\t Train Set - 156207\n",
      "\t Val Set - 66946\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=dataset_configs[\"train_test_split\"])\n",
    "train_dataset, validation_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "print(\"Lengths\")\n",
    "print(\"\\t Train Set - {}\".format(len(train_dataset)))\n",
    "print(\"\\t Val Set - {}\".format(len(validation_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hffXHORwnMrd"
   },
   "source": [
    "Having a look at a dataset sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AyVBWfpoKQXI",
    "outputId": "2d8fb58f-8dc5-4f94-d9f9-d2622044159c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a Datapoint \n",
      "\n",
      "{'id': '123598', 'translation': {'de': 'Nachrichtenberichte und â€žAnalysenâ€œ der staatlich kontrollierten Sender in Russland und Georgien, die ein negatives Image â€ždes Feindesâ€œ zeichnen, dienen lediglich dazu, die Kluft zwischen den ethnischen Gruppen noch zu vertiefen.', 'en': 'News reports and â€œanalysisâ€ by state-controlled channels in both Russia and Georgia that promote negative images of â€œthe enemyâ€ serve only to widen the gap between ethnic groups.'}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of a Datapoint \\n\")\n",
    "print(train_dataset[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SOKRE0KnTMF"
   },
   "source": [
    "### Tokenizer\n",
    "\n",
    "The tokenizer needs to be defined to convert the input from strings to token ids. The Machine Translation tokenizers need additional parameters about the source language and target language, refer [here](https://huggingface.co/transformers/model_doc/mbart.html#transformers.MBartTokenizer) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p-o1azyZVQnW"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config[\"tokenizer_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3csI0cMn-Bj"
   },
   "source": [
    "## Dataset Class \n",
    "Tokenizes the data and returns a dictionary with inputs and targets.\n",
    "\n",
    "If you want to train on a subset of the data - modify the ``train_dataset_length`` and ``validation_dataset_length`` in the dataset configs. Keep them as -1 for taking the whole length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SJ5tkvn9y8Uo"
   },
   "outputs": [],
   "source": [
    "class TransformerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, data, src_text_id, tgt_text_id, tokenizer, max_length, length_dataset\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.src_text_id = src_text_id\n",
    "        self.tgt_text_id = tgt_text_id\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.length_dataset = length_dataset if length_dataset != -1 else len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_prefix = \"translate {} to {}: \".format(dataset_configs[\"source_language\"], dataset_configs[\"target_language\"])\n",
    "        src_text = [task_prefix + str(self.data[idx][\"translation\"][self.src_text_id])]\n",
    "\n",
    "        tgt_text = [str(self.data[idx][\"translation\"][self.tgt_text_id])]\n",
    "        input_txt_tokenized = self.tokenizer(\n",
    "            src_text, max_length=self.max_length, padding=\"max_length\", truncation=True\n",
    "        )\n",
    "\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            tgt_text_tokenized = self.tokenizer(\n",
    "                tgt_text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "\n",
    "        # The pad token in target is replaced with -100 so that it doesn't get added to loss.\n",
    "        tgt_text_tokenized = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "            for label in tgt_text_tokenized.input_ids\n",
    "        ]\n",
    "\n",
    "        input_txt_tokenized.update({\"tgt\": tgt_text_tokenized[0]})\n",
    "\n",
    "        batch = {\n",
    "            k: torch.tensor(v).squeeze(0) for (k, v) in input_txt_tokenized.items()\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rdf-SYMp1oEs"
   },
   "outputs": [],
   "source": [
    "train_data = TransformerDataset(\n",
    "    train_dataset,\n",
    "    dataset_configs[\"source_text_id\"],\n",
    "    dataset_configs[\"target_text_id\"],\n",
    "    tokenizer,\n",
    "    dataset_configs[\"max_length\"],\n",
    "    dataset_configs[\"train_dataset_length\"],\n",
    ")\n",
    "val_data = TransformerDataset(\n",
    "    validation_dataset,\n",
    "    dataset_configs[\"source_text_id\"],\n",
    "    dataset_configs[\"target_text_id\"],\n",
    "    tokenizer,\n",
    "    dataset_configs[\"max_length\"],\n",
    "    dataset_configs[\"validation_dataset_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C19xqP_h9gj"
   },
   "source": [
    "## Trainer\n",
    "The trainer takes a batch of input and passes them through the model (along with targets in this case) and gets the loss.\n",
    "\n",
    "#### Mixed Precision\n",
    "The forward pass is wrapped in the autocast context manager for mixed precision training. It's turned off in this example as there won't be any memory advantages with ``batch_size`` 1 or 2. Change the ``with_amp`` flag in config to turn it on.\n",
    "\n",
    "#### Gradient Accumulation\n",
    "Gradient accumulation is implemented as batch size of 1 would lead to noisy updates otherwise. Check the ``accumulation_steps`` variable in config to define the number of steps to accumulate the gradient. \n",
    "\n",
    "#### Trainer Handlers\n",
    "Handlers can be defined and attached directly to the trainer engine. Here we also make use of a special function : `setup_common_training_handlers` which has a lot of the commonly used, useful handlers (like `save_every_iters`, `clear_cuda_cache` etc) already defined. To know more about this function, refer to the docs [here](https://pytorch.org/ignite/contrib/engines.html#ignite.contrib.engines.common.setup_common_training_handlers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hTIlKn9CJGMC"
   },
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "def create_trainer(model, optimizer, with_amp, train_sampler, logger):\n",
    "    device = idist.device()\n",
    "    scaler = GradScaler(enabled=with_amp)\n",
    "\n",
    "    def train_step(engine, batch):\n",
    "        model.train()\n",
    "\n",
    "        if batch[\"tgt\"].device != device:\n",
    "            batch = {\n",
    "                k: v.to(device, non_blocking=True, dtype=torch.long)\n",
    "                for (k, v) in batch.items()\n",
    "            }\n",
    "\n",
    "        src_ids = batch[\"input_ids\"]\n",
    "        src_attention_mask = batch[\"attention_mask\"]\n",
    "        tgt = batch[\"tgt\"]\n",
    "\n",
    "        with autocast(enabled=with_amp):\n",
    "            y = model(input_ids=src_ids, attention_mask=src_attention_mask, labels=tgt)\n",
    "            loss = y[\"loss\"]\n",
    "            loss /= config[\"accumulation_steps\"]\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if engine.state.iteration % config[\"accumulation_steps\"] == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        return {\"batch loss\": loss.item()}\n",
    "\n",
    "    trainer = Engine(train_step)\n",
    "    trainer.logger = logger\n",
    "\n",
    "    metric_names = [\"batch loss\"]\n",
    "\n",
    "    common.setup_common_training_handlers(\n",
    "        trainer=trainer,\n",
    "        train_sampler=train_sampler,\n",
    "        output_names=metric_names,\n",
    "        clear_cuda_cache=False,\n",
    "        with_pbars=False,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf5lFIiwwack"
   },
   "source": [
    "## Evaluator\n",
    "Similar to trainer we create an evaluator for validation step. Here we calculate metrics (like Bleu Score). To do this Bleu score requires the sentences and not the logits. the ``ids_to_clean_text`` function is used to do that.\n",
    "\n",
    "The ``print_output_every`` flag can be changed if you want to change the frequency of printing output sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AphMNHAJoXJV"
   },
   "outputs": [],
   "source": [
    "# Let's now setup evaluator engine to perform model's validation and compute metrics\n",
    "def create_evaluator(model, tokenizer, metrics, with_amp,logger, tag=\"val\"):\n",
    "    device = idist.device()\n",
    "\n",
    "    def ids_to_clean_text(generated_ids):\n",
    "        gen_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        return list(map(str.strip, gen_text))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_step(engine, batch):\n",
    "        model.eval()\n",
    "\n",
    "        if batch[\"tgt\"].device != device:\n",
    "            batch = {\n",
    "                k: v.to(device, non_blocking=True, dtype=torch.long)\n",
    "                for (k, v) in batch.items()\n",
    "            }\n",
    "\n",
    "        src_ids = batch[\"input_ids\"]\n",
    "        src_attention_mask = batch[\"attention_mask\"]\n",
    "        tgt = batch[\"tgt\"]\n",
    "        if idist.get_world_size() > 1:\n",
    "            y_pred = model.module.generate(input_ids=src_ids, attention_mask=src_attention_mask)\n",
    "        else:   \n",
    "            y_pred = model.generate(input_ids=src_ids, attention_mask=src_attention_mask)\n",
    "\n",
    "        tgt = torch.where(tgt != -100, tgt, tokenizer.pad_token_id)\n",
    "\n",
    "        preds = ids_to_clean_text(y_pred)\n",
    "        tgt = ids_to_clean_text(tgt)\n",
    "        preds = [_preds.split() for _preds in preds]\n",
    "        tgt = [[_tgt.split()] for _tgt in tgt]\n",
    "        \n",
    "        if engine.state.iteration % config[\"print_output_every\"] == 0:\n",
    "            logger.info(f'\\n Preds : {\" \".join(preds[0])} \\n')\n",
    "            logger.info(f'\\n Target : {\" \".join(tgt[0][0])} \\n')\n",
    "        return preds, tgt\n",
    "\n",
    "    evaluator = Engine(evaluate_step)\n",
    "\n",
    "    for name, metric in metrics.items():\n",
    "        metric.attach(evaluator, name)\n",
    "\n",
    "    return evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6psomYz0w7GU"
   },
   "source": [
    "## Initializing Functions\n",
    "\n",
    "\n",
    "Here we initialize the model and optimizer. \\\n",
    "The ``get_dataloader`` returns dataloaders for train and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7aNi7qao66YQ"
   },
   "outputs": [],
   "source": [
    "def freeze_params(model):\n",
    "    for par in model.parameters():\n",
    "        par.requires_grad = False\n",
    "\n",
    "\n",
    "def initialize():\n",
    "    model = T5ForConditionalGeneration.from_pretrained(config[\"model_name\"])\n",
    "    lr = config[\"learning_rate\"] * idist.get_world_size()\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": config[\"weight_decay\"],\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    if config[\"freeze_encoder\"]:\n",
    "        freeze_params(model.get_encoder())\n",
    "\n",
    "    model = idist.auto_model(model)\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    optimizer = idist.auto_optim(optimizer)\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CThVRupzsxuQ"
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(train_dataset, val_dataset):\n",
    "    # Setup data loader also adapted to distributed config: nccl, gloo, xla-tpu\n",
    "    train_loader = idist.auto_dataloader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = idist.auto_dataloader(\n",
    "        val_dataset,\n",
    "        batch_size=2 * config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEI8MWqlx8WY"
   },
   "source": [
    "## Logging Handlers\n",
    "This step is optional, however, we can pass a ``setup_logger()`` object to ``log_basic_info()`` and log all basic information such as different versions, current configuration, device and backend used by the current process (identified by its local rank), and number of processes (``world size``). idist (``ignite.distributed``) provides several utility functions like ``get_local_rank()``, ``backend()``, ``get_world_size()``, etc. to make this possible.\n",
    "\n",
    "The ``log_metrics_eval`` is used to log metrics and evaluation time for running evaluation.\n",
    "\n",
    "The ``get_save_handler`` is used to save the model to the output path whenever it is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6G1kyCtWxbZy"
   },
   "outputs": [],
   "source": [
    "def log_metrics_eval(logger, epoch, elapsed, tag, metrics):\n",
    "    metrics_output = \"\\n\".join([f\"\\t{k}: {v}\" for k, v in metrics.items()])\n",
    "    logger.info(\n",
    "        f\"\\nEpoch {epoch} - Evaluation time (seconds): {elapsed:.2f} - {tag} metrics:\\n {metrics_output}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def log_basic_info(logger, config):\n",
    "    logger.info(f\"Train on CIFAR10\")\n",
    "    logger.info(f\"- PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"- Ignite version: {ignite.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        # explicitly import cudnn as torch.backends.cudnn can not be pickled with hvd spawning procs\n",
    "        from torch.backends import cudnn\n",
    "\n",
    "        logger.info(\n",
    "            f\"- GPU Device: {torch.cuda.get_device_name(idist.get_local_rank())}\"\n",
    "        )\n",
    "        logger.info(f\"- CUDA version: {torch.version.cuda}\")\n",
    "        logger.info(f\"- CUDNN version: {cudnn.version()}\")\n",
    "\n",
    "    logger.info(\"\\n\")\n",
    "    logger.info(\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        logger.info(f\"\\t{key}: {value}\")\n",
    "    logger.info(\"\\n\")\n",
    "\n",
    "    if idist.get_world_size() > 1:\n",
    "        logger.info(\"\\nDistributed setting:\")\n",
    "        logger.info(f\"\\tbackend: {idist.backend()}\")\n",
    "        logger.info(f\"\\tworld size: {idist.get_world_size()}\")\n",
    "        logger.info(\"\\n\")\n",
    "\n",
    "\n",
    "def get_save_handler(config):\n",
    "    return DiskSaver(config[\"output_path_\"], require_empty=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx5oxrigyW5N"
   },
   "source": [
    "## Begin Training\n",
    "This is where the main logic resides, i.e. we will call all the above functions from within here:\n",
    "\n",
    "1. Basic Setup \n",
    "      1. We set a ``manual_seed()`` and ``setup_logger()``, then log all basic information. \n",
    "      2. Initialise dataloaders, model and optimizer. \n",
    "2. We use the above objects to create a trainer.\n",
    "3. Evaluator \n",
    "      1. Define some relevant Ignite metrics like ``Bleu()``. \n",
    "      2. Create evaluator: ``evaluator`` to compute metrics on the ``val_dataloader``. \n",
    "      3. Define ``run_validation()`` to compute metrics on both dataloaders and log them. Then we attach this function to trainer to run after epochs.\n",
    "4. Setup TensorBoard logging using ``setup_tb_logging()`` on the master process for the evaluators so that validation metrics along with the learning rate can be logged.\n",
    "5. Define a ``Checkpoint()`` object to store the two best models (``n_saved``) by validation accuracy (defined in metrics as ``Bleu()``) and attach it to val_evaluator so that it can be executed everytime ``evaluator`` runs.\n",
    "6. Try training on ``train_loader`` for ``num_epochs``\n",
    "7. Close Tensorboard logger once training is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4l0aDOM6EikW"
   },
   "outputs": [],
   "source": [
    "def training(local_rank):\n",
    "    rank = idist.get_rank()\n",
    "    manual_seed(config[\"seed\"] + rank)\n",
    "    device = idist.device()\n",
    "\n",
    "    logger = setup_logger(name=\"NMT\", distributed_rank=local_rank)\n",
    "    log_basic_info(logger, config)\n",
    "\n",
    "    train_loader, val_loader = get_dataloaders(train_data, val_data)\n",
    "    model, optimizer = initialize()\n",
    "\n",
    "    trainer = create_trainer(\n",
    "        model, optimizer, config[\"with_amp\"], train_loader.sampler, logger\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"bleu\": Bleu(ngram=4, smooth=\"smooth1\", average=\"micro\"),\n",
    "        \"bleu_smooth_2\": Bleu(ngram=4, smooth=\"smooth2\", average=\"micro\"),\n",
    "    }\n",
    "\n",
    "    evaluator = create_evaluator(\n",
    "        model, tokenizer, metrics, config[\"with_amp\"], logger, tag=\"val\"\n",
    "    )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED(every=1) | Events.COMPLETED | Events.STARTED)\n",
    "    def run_validation(engine):\n",
    "        epoch = trainer.state.epoch\n",
    "        state = evaluator.run(val_loader)\n",
    "        log_metrics_eval(\n",
    "            logger, epoch, state.times[\"COMPLETED\"], \"Validation\", state.metrics\n",
    "        )\n",
    "\n",
    "    if rank == 0:\n",
    "        now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        folder_name = f\"Translation_Model_backend-{idist.backend()}-{idist.get_world_size()}_{now}\"\n",
    "        output_path = Path(config[\"output_path_\"]) / folder_name\n",
    "        if not output_path.exists():\n",
    "            output_path.mkdir(parents=True)\n",
    "\n",
    "        logger.info(f\"Output path: {output_path}\")\n",
    "\n",
    "        evaluators = {\"val\": evaluator}\n",
    "        tb_logger = common.setup_tb_logging(\n",
    "            config[\"output_path_\"], trainer, optimizer, evaluators=evaluators\n",
    "        )\n",
    "\n",
    "    best_model_handler = Checkpoint(\n",
    "        {\"model\": model},\n",
    "        get_save_handler(config),\n",
    "        filename_prefix=\"best\",\n",
    "        n_saved=2,\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "        score_name=\"val_bleu\",\n",
    "        score_function=Checkpoint.get_default_score_fn(\"bleu\"),\n",
    "    )\n",
    "    evaluator.add_event_handler(Events.COMPLETED, best_model_handler)\n",
    "\n",
    "    try:\n",
    "        state = trainer.run(\n",
    "            train_loader,\n",
    "            max_epochs=config[\"num_epochs\"],\n",
    "            epoch_length=config[\"epoch_length\"],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.exception(\"\")\n",
    "        raise e\n",
    "\n",
    "    if rank == 0:\n",
    "        tb_logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPQfU0X8BxSk"
   },
   "source": [
    "## Running\n",
    "To run with TPU change ``backend`` to \"xla-tpu\" and ``nproc_per_node`` to 1 or 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4rGWEfQIrh5"
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    with idist.Parallel(backend=None, nproc_per_node=None) as parallel:\n",
    "        parallel.run(training)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Machine_Translation_using_PyTorch_Ignite.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09299b671401414897a169c8ca6fab5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bc960960a7b4a71bf723767558b06fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2de127df69f546f2b8ffe1b729b747ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e27f0b174a44917b2081d88c31177c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "440e4631c3ec4673b8378513cdb5ef88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "450f239f4b83439ebef94e44e655b41c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ab0ef2ca6a7481aa3bb1561ac179674": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e27f0b174a44917b2081d88c31177c6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2bc960960a7b4a71bf723767558b06fe",
      "value": "100%"
     }
    },
    "8e63d03c0e014272b66cfd25d1cba5ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f865438773d4c9ca8c12cae83df79b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ab0ef2ca6a7481aa3bb1561ac179674",
       "IPY_MODEL_c0157213cc6e44f6aab3f8087157d6b5",
       "IPY_MODEL_cf1aff588ac34b97a0ecd46a11ca961c"
      ],
      "layout": "IPY_MODEL_450f239f4b83439ebef94e44e655b41c"
     }
    },
    "c0157213cc6e44f6aab3f8087157d6b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09299b671401414897a169c8ca6fab5c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2de127df69f546f2b8ffe1b729b747ac",
      "value": 1
     }
    },
    "cf1aff588ac34b97a0ecd46a11ca961c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e63d03c0e014272b66cfd25d1cba5ea",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_440e4631c3ec4673b8378513cdb5ef88",
      "value": " 1/1 [00:00&lt;00:00, 19.50it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
