{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "idist-collective-communication.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w-QlZE9mvdY"
      },
      "source": [
        "<!-- ---\n",
        "title: Collective Communication with Ignite\n",
        "weight: 5\n",
        "date: 2021-10-5\n",
        "downloads: true\n",
        "sidebar: true\n",
        "tags:\n",
        "  - idist\n",
        "  - all_gather\n",
        "  - all_reduce\n",
        "  - broadcast\n",
        "  - barrier\n",
        "--- -->\n",
        "\n",
        "# Collective Communication with Ignite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJgTaKWU8Doq"
      },
      "source": [
        "In this tutorial, we will see how to use advanced distributed functions like `all_reduce()`, `all_gather()`, `broadcast()` and `barrier()`. We will discuss unique use cases for all of them and represent them visually.\n",
        "\n",
        "<!--more-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhiy_ylcn2GD"
      },
      "source": [
        "## Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zevsoVQ4nx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1692af67-6cf5-4f6f-b06e-77838bcf789a"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 31.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrvIsRKQn42e"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMphyBmmmvdw",
        "pycharm": {
          "is_executing": false
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "import ignite.distributed as idist"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2KPTliNC2r9"
      },
      "source": [
        "## All Reduce\n",
        "\n",
        "![All Reduce Diagram](https://github.com/pytorch-ignite/examples/blob/add-collective-comm-nb/tutorials/assets/all-reduce.png?raw=1)\n",
        "\n",
        "The [`all_reduce()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.all_reduce) method is used to collect specified tensors from each process and make them available on every node then perform a specified operation (sum, product, min, max, etc) on them. Let's spawn 3 processes with ranks 0, 1 and 2 and define a `tensor` on all of them. If we performed `all_reduce` with the operation SUM on `tensor` then `tensor` on all ranks will be gathered, added and stored in `tensor` as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHT6EftPOwUq"
      },
      "source": [
        "def all_reduce_example(local_rank):\n",
        "    tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * local_rank\n",
        "    print(f\"Rank {local_rank}, Initial value: {tensor}\")\n",
        "\n",
        "    idist.all_reduce(tensor, op=\"SUM\")\n",
        "    print(f\"Rank {local_rank}, After performing all_reduce: {tensor}\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz5mYRS76gjm"
      },
      "source": [
        "We can use [idist.Parallel](https://pytorch.org/ignite/generated/ignite.distributed.launcher.Parallel.html#ignite.distributed.launcher.Parallel) to spawn 3 processes and execute the above function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCc1OxDg3X38",
        "outputId": "c2453477-948f-4fc7-f0ed-59f8d63bfaf0"
      },
      "source": [
        "spawn_kwargs = {}\n",
        "spawn_kwargs[\"start_method\"] = \"fork\"\n",
        "spawn_kwargs[\"nproc_per_node\"] = 3\n",
        "\n",
        "with idist.Parallel(backend=\"gloo\", **spawn_kwargs) as parallel:\n",
        "    parallel.run(all_reduce_example)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-26 13:57:43,725 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'gloo'\n",
            "2021-10-26 13:57:43,729 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 3\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "\tstart_method: fork\n",
            "2021-10-26 13:57:43,731 ignite.distributed.launcher.Parallel INFO: Spawn function '<function all_reduce_example at 0x7f473d091200>' in 3 processes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 0, Initial value: tensor([1, 2])\n",
            "Rank 2, Initial value: tensor([5, 6])\n",
            "Rank 1, Initial value: tensor([3, 4])\n",
            "Rank 2, After performing all_reduce: tensor([ 9, 12])\n",
            "Rank 1, After performing all_reduce: tensor([ 9, 12])\n",
            "Rank 0, After performing all_reduce: tensor([ 9, 12])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-26 13:57:43,897 ignite.distributed.launcher.Parallel INFO: End of run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLntezo0NhQg"
      },
      "source": [
        "Now let's assume a more real world scenario - You need to find the average of all the gradients available on different processes. \n",
        "\n",
        "> First, we get the number of GPUs available, with the get_world_size method. Then, for every model parameter, we do the following:\n",
        ">\n",
        ">    1. Gather the gradients on each process\n",
        ">    2. Apply the sum operation on the gradients\n",
        ">    3. Divide by the world size to average them\n",
        ">\n",
        "> Finally, we can go on to update the model parameters using the averaged gradients!\n",
        ">\n",
        "> -- <cite>[Distributed Deep Learning 101: Introduction](https://towardsdatascience.com/distributed-deep-learning-101-introduction-ebfc1bcd59d9)</cite>\n",
        "\n",
        "You can get the number of GPUs (processes) available using another helper method [`idist.get_world_size()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.get_world_size) and then use `all_reduce()` to collect the gradients and apply the SUM operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j_ErUWhHpTl"
      },
      "source": [
        "def average_gradients(model):\n",
        "    num_processes = idist.get_world_size()\n",
        "    for param in model.parameters():\n",
        "        idist.all_reduce(param.grad.data, op=\"SUM\")\n",
        "        param.grad.data = param.grad.data / num_processes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w9oIcIiC6_4"
      },
      "source": [
        "## All Gather\n",
        "\n",
        "![All Gather Diagram](https://github.com/pytorch-ignite/examples/blob/add-collective-comm-nb/tutorials/assets/all-gather.png?raw=1)\n",
        "\n",
        "The [`all_gather()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.all_gather) method is used when you just want to collect a tensor, number or string across all participating processes. As a basic example, suppose you have to collect all the different values stored in `num` on all ranks. You can achieve this by using `all_gather` as below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1ZD4jPK5CVm"
      },
      "source": [
        "def all_gather_example(local_rank):\n",
        "    num = 2.0 * idist.get_rank()\n",
        "    print(f\"Rank {local_rank}, Initial value: {num}\")\n",
        "\n",
        "    all_nums = idist.all_gather(num)\n",
        "    print(f\"Rank {local_rank}, After performing all_gather: {all_nums}\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyGu-S7I5Uzw",
        "outputId": "a5afd94d-3b97-4cf5-ecda-14890d754bf9"
      },
      "source": [
        "with idist.Parallel(backend=\"gloo\", **spawn_kwargs) as parallel:\n",
        "    parallel.run(all_gather_example)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-26 14:00:42,890 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'gloo'\n",
            "2021-10-26 14:00:42,893 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 3\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "\tstart_method: fork\n",
            "2021-10-26 14:00:42,895 ignite.distributed.launcher.Parallel INFO: Spawn function '<function all_gather_example at 0x7f473d091e60>' in 3 processes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 1, Initial value: 2.0\n",
            "Rank 0, Initial value: 0.0\n",
            "Rank 2, Initial value: 4.0\n",
            "Rank 1, After performing all_gather: [0.0, 2.0, 4.0]\n",
            "Rank 2, After performing all_gather: [0.0, 2.0, 4.0]\n",
            "Rank 0, After performing all_gather: [0.0, 2.0, 4.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-26 14:00:43,070 ignite.distributed.launcher.Parallel INFO: End of run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdTcvz4pStfD"
      },
      "source": [
        "Now let's assume you need to gather the predicted values which are distributed across all the processes on the main process so you could store them to a file. Here is how you can do it: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRvgasbLC8Ne"
      },
      "source": [
        "def write_preds_to_file(predictions, filename):\n",
        "    prediction_tensor = torch.tensor(predictions)\n",
        "    prediction_tensor = idist.all_gather(prediction_tensor)\n",
        "\n",
        "    if idist.get_rank() == 0:\n",
        "        torch.save(prediction_tensor, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib9PfbieNMw_"
      },
      "source": [
        "**Note:** In the above example, only the main process required the gathered values and not all the processes. This can be also be done via the `gather()` method, however one of the backends [`nccl` does not support `gather()`](https://pytorch.org/docs/stable/distributed.html) hence we had to use `all_gather()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fsu-NybC8t1"
      },
      "source": [
        "## Broadcast\n",
        "\n",
        "![Broadcast Diagram](https://github.com/pytorch-ignite/examples/blob/add-collective-comm-nb/tutorials/assets/broadcast.png?raw=1)\n",
        "\n",
        "The [`broadcast()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.broadcast) method copies a tensor, float or string from a source process to all the other processes. For example, you need to send a message from rank 0 to all other ranks. You can do this by creating the actual message on rank 0 and a placeholder on all other ranks, then broadcast the message mentioning a source rank. You can also use `safe_mode=True` in case the placeholder is not defined on all ranks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWowyRRw55XM"
      },
      "source": [
        "def broadcast_example(local_rank):\n",
        "    message = f\"hello from rank {idist.get_rank()}\"\n",
        "    print(f\"Rank {local_rank}, Initial value: {message}\")\n",
        "\n",
        "    message = idist.broadcast(message, src=0)\n",
        "    print(f\"Rank {local_rank}, After performing broadcast: {message}\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYXfJFBfUYiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e54113f-40c3-4405-ee47-960963a52ca1"
      },
      "source": [
        "with idist.Parallel(backend=\"gloo\", **spawn_kwargs) as parallel:\n",
        "    parallel.run(broadcast_example)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-26 14:02:36,158 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'gloo'\n",
            "2021-10-26 14:02:36,162 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 3\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "\tstart_method: fork\n",
            "2021-10-26 14:02:36,166 ignite.distributed.launcher.Parallel INFO: Spawn function '<function broadcast_example at 0x7f473cff67a0>' in 3 processes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 2, Initial value: hello from rank 2\n",
            "Rank 1, Initial value: hello from rank 1\n",
            "Rank 0, Initial value: hello from rank 0\n",
            "Rank 1, After performing broadcast: hello from rank 0\n",
            "Rank 2, After performing broadcast: hello from rank 0\n",
            "Rank 0, After performing broadcast: hello from rank 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-26 14:02:36,332 ignite.distributed.launcher.Parallel INFO: End of run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVXVi2rcTz9X"
      },
      "source": [
        "For a real world use case, let's assume you need to gather the predicted and actual values from all the processes on rank 0 for computing a metric and avoiding a memory error. You can do do this by first using `all_gather()`, then computing the metric and finally using `broadcast()` to share the result with all processes. `src` below refers to the rank of the source process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aXwKnrTC96T"
      },
      "source": [
        "def compute_metric(prediction_tensor, target_tensor):\n",
        "\n",
        "    prediction_tensor = idist.all_gather(prediction_tensor)\n",
        "    target_tensor = idist.all_gather(target_tensor)\n",
        "\n",
        "    result = 0.0\n",
        "    if idist.get_rank() == 0:\n",
        "        result = compute_fn(prediction_tensor, target_tensor)\n",
        "\n",
        "    result = idist.broadcast(result, src=0)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ma7l5cDIuC"
      },
      "source": [
        "## Barrier\n",
        "\n",
        "The [`barrier()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.barrier) method helps synchronize all processes. For example - while downloading data during training, we have to make sure only the main process (`rank = 0`) downloads the datasets to prevent the sub processes (`rank > 0`) from downloading the same file to the same path at the same time. This way all sub processes get a copy of this already downloaded dataset. This is where we can utilize `barrier()` to make the sub processes wait until the main process downloads the datasets. Once that is done, all the subprocesses instantiate the datasets, while the main process waits. Finally, all the processes are synced up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XInr0zlhDJl6"
      },
      "source": [
        "def get_datasets(config):\n",
        "    if idist.get_local_rank() > 0:\n",
        "        idist.barrier()\n",
        "\n",
        "    train_dataset, test_dataset = get_train_test_datasets(config[\"data_path\"])\n",
        "\n",
        "    if idist.get_local_rank() == 0:\n",
        "        idist.barrier()\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}